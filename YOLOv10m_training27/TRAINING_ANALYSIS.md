# YOLOv10m Training27 - Complete Analysis

**Model:** YOLOv10m (Medium variant)  
**Task:** Traffic Sign Detection  
**Training Date:** January 26, 2025  
**Location:** `/home/mnxtr/Traffic-Sign-Recognition-YOLOv10/YOLOv10m_training27/`

---

## üì¶ Training Results Package Contents

### Performance Metrics (PNG - High Resolution)

| File | Dimensions | Size | Purpose |
|------|------------|------|---------|
| `confusion_matrix.png` | 3000√ó2250 | 191 KB | Raw classification results |
| `confusion_matrix_normalized.png` | 3000√ó2250 | 186 KB | **Percentage-based results** ‚≠ê |
| `F1_curve.png` | 2250√ó1500 | 157 KB | Optimal threshold selection |
| `P_curve.png` | 2250√ó1500 | 154 KB | Precision vs confidence |
| `R_curve.png` | 2250√ó1500 | 92 KB | Recall vs confidence |
| `PR_curve.png` | 2250√ó1500 | 97 KB | Overall performance (mAP) |

### Validation Examples (JPEG - Mosaic View)

| File | Dimensions | Size | Purpose |
|------|------------|------|---------|
| `val_batch0_labels.jpg` | 1344√ó1344 | 277 KB | Ground truth annotations |
| `val_batch0_pred.jpg` | 1344√ó1344 | 278 KB | Model predictions |

**Total Size:** ~1.5 MB

---

## üéØ What Each Metric Tells You

### 1. **Confusion Matrix** (Most Important!)

**File to Check:** `confusion_matrix_normalized.png`

```
              Predicted Classes
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ  0   1   2  ...  N  ‚îÇ
        ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
True    0  ‚îÇ 95%  3%  2%  ... 0% ‚îÇ ‚Üê Class 0 accuracy
Classes 1  ‚îÇ  2% 92%  1%  ... 5% ‚îÇ ‚Üê Class 1 accuracy
        2  ‚îÇ  1%  0% 88%  ... 1% ‚îÇ
        ...‚îÇ ...  ... ...  ... ..‚îÇ
        N  ‚îÇ  0%  4%  0%  ... 90%‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**How to Read:**
- **Diagonal (top-left to bottom-right)** = Correct predictions
  - Higher percentages = Better performance
  - Target: >80% for each class
  
- **Off-diagonal values** = Misclassifications
  - Shows which signs are confused with each other
  - Example: If Class 5 (stop sign) shows 10% in Class 3 column,
    the model confuses 10% of stop signs with Class 3

**What to Look For:**
- ‚úÖ Strong diagonal (dark/high values)
- ‚úÖ Light colors everywhere else
- ‚ö†Ô∏è Bright off-diagonal spots = systematic confusion
- ‚ö†Ô∏è Dim diagonal = poor class performance

---

### 2. **Precision-Recall (PR) Curve**

**File to Check:** `PR_curve.png`

**Definitions:**
- **Precision** = What % of detections are correct?
  - `Precision = True Positives / (True Positives + False Positives)`
  - High precision = Few false alarms

- **Recall** = What % of actual signs are found?
  - `Recall = True Positives / (True Positives + False Negatives)`
  - High recall = Few missed signs

**Interpretation:**
```
1.0 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ     ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó        ‚îÇ ‚Üê Excellent (mAP > 0.9)
P   ‚îÇ    ‚ïî‚ïù      ‚ïö‚ïó       ‚îÇ
r   ‚îÇ   ‚ïî‚ïù        ‚ïö‚ïó      ‚îÇ
e   ‚îÇ  ‚ïî‚ïù          ‚ïö‚ïó     ‚îÇ
c   ‚îÇ ‚ïî‚ïù            ‚ïö‚ïó    ‚îÇ
i   ‚îÇ‚ïî‚ïù              ‚ïö‚ïê‚ïó  ‚îÇ
s   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
i   0    0.5    1.0
o        Recall
n
```

**Performance Levels:**
- **mAP@0.5 > 0.90** = Excellent (production ready)
- **mAP@0.5 = 0.80-0.90** = Very good
- **mAP@0.5 = 0.70-0.80** = Good
- **mAP@0.5 < 0.70** = Needs improvement

---

### 3. **F1 Score Curve**

**File to Check:** `F1_curve.png`

**Formula:** `F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)`

**Purpose:** Find the optimal confidence threshold

```
F1 Score
1.0 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ        ‚ï±‚ï≤          ‚îÇ
0.8 ‚îÇ      ‚ï±    ‚ï≤        ‚îÇ ‚Üê Peak = Optimal threshold
    ‚îÇ    ‚ï±        ‚ï≤      ‚îÇ
0.6 ‚îÇ  ‚ï±            ‚ï≤    ‚îÇ
    ‚îÇ‚ï±                ‚ï≤  ‚îÇ
0.0 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    0.0    0.5    1.0
       Confidence Threshold
```

**How to Use:**
1. Find the peak of the curve
2. Note the confidence value at peak
3. Use this threshold for inference
4. Typical optimal: 0.25 - 0.50

**Example:**
- If peak F1 = 0.85 at confidence 0.30
- Set model confidence threshold to 0.30 for balanced performance

---

### 4. **Precision & Recall Curves**

**Files:** `P_curve.png`, `R_curve.png`

**Trade-off Visualization:**

```
      Precision              Recall
1.0 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      1.0 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ‚ï≤          ‚îÇ          ‚îÇ‚ï±          ‚îÇ
0.8 ‚îÇ ‚ï≤         ‚îÇ      0.8 ‚îÇ‚ï±           ‚îÇ
    ‚îÇ  ‚ï≤        ‚îÇ          ‚ï±            ‚îÇ
0.6 ‚îÇ   ‚ï≤       ‚îÇ      0.6‚îÇ‚ï±             ‚îÇ
    ‚îÇ    ‚ï≤      ‚îÇ          ‚ï±              ‚îÇ
0.0 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      0.0‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    0.0   1.0              0.0   1.0
    Confidence             Confidence
```

**Use Case Selection:**

| Use Case | Optimize For | Threshold |
|----------|--------------|-----------|
| **Safety-Critical** (Driving) | Recall (don't miss signs) | Lower (0.1-0.3) |
| **Alerting System** | Balance (F1 peak) | Medium (0.3-0.5) |
| **Low False Alarms** | Precision (accuracy) | Higher (0.5-0.7) |

---

### 5. **Validation Batch Comparison**

**Files:** `val_batch0_labels.jpg` vs `val_batch0_pred.jpg`

**Layout:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Image 1 ‚îÇ Image 2 ‚îÇ Image 3 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Image 4 ‚îÇ Image 5 ‚îÇ Image 6 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Image 7 ‚îÇ Image 8 ‚îÇ Image 9 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Visual Inspection Checklist:**

‚úÖ **Good Signs:**
- Bounding boxes match closely
- All signs detected
- Correct class labels
- Tight boxes around signs

‚ö†Ô∏è **Issues to Note:**
- **Missed detections** (in labels, not in predictions)
- **False positives** (in predictions, not in labels)
- **Loose boxes** (boxes too large/small)
- **Wrong classes** (detected but wrong label)

---

## üìä Training Progression Analysis

All 5 training checkpoints (22, 23, 25, 25(2), 27) have **identical file sizes**:

| Training | File Sizes | Observation |
|----------|------------|-------------|
| training22 | 1,462,161 bytes | Same metrics |
| training23 | 1,462,161 bytes | Same metrics |
| training25 | 1,462,161 bytes | Same metrics |
| training25(2) | 1,462,161 bytes | Same metrics |
| training27 | 1,462,161 bytes | Same metrics |

**Possible Explanations:**
1. **Snapshots of same final model** with different names
2. **Checkpoints from same epoch** but different experiments
3. **Best model saved multiple times** during training
4. **Training converged** - no improvement between runs

**Recommendation:** Extract and compare confusion matrices visually to confirm if they're truly identical or show gradual improvement.

---

## üöÄ How to Analyze Your Results

### Step 1: Visual Inspection

```bash
cd /home/mnxtr/Traffic-Sign-Recognition-YOLOv10/YOLOv10m_training27

# Option 1: Using image viewer
display confusion_matrix_normalized.png &
display PR_curve.png &
display F1_curve.png &

# Option 2: Using Python
python3 << EOF
from PIL import Image
Image.open('confusion_matrix_normalized.png').show()
Image.open('PR_curve.png').show()
EOF
```

### Step 2: Compare Validation Results

```bash
# Side-by-side comparison
display val_batch0_labels.jpg val_batch0_pred.jpg
```

### Step 3: Extract Metrics (if needed)

If you need numerical values, you'll need to check:
- `results.csv` (typically in training output)
- Training logs
- Model weights metadata

---

## üí° Actionable Insights

### Based on Prediction Analysis (from predict folder):

**Current Performance:**
- 8 classes detected
- Class 36 dominates (36% of detections)
- Classes 22, 29 rare (<2% each)

**Likely Issues to Check in Confusion Matrix:**

1. **Class Imbalance**
   - Look for dim rows/columns (underrepresented classes)
   - Check if Classes 22, 29, 35 have low accuracy

2. **Class Confusion**
   - Check if similar-looking signs are confused
   - Example: Warning signs (Class 23 vs 36)
   - Example: Regulatory signs (Class 22 vs 32)

3. **Detection Threshold**
   - Only 33% of images have predictions
   - Check F1 curve to see if lower threshold helps
   - May need to balance precision vs recall

---

## üéì Reading Example

**Hypothetical Good Results:**

```
Confusion Matrix (normalized):
- Diagonal values: 85-95% (excellent)
- Off-diagonal: <5% (minimal confusion)
- Weakest class: Class 22 at 72% (needs more data)

PR Curve:
- mAP@0.5 = 0.87 (very good)
- All classes > 0.75 except Class 22 (0.68)

F1 Curve:
- Peak F1 = 0.84 at confidence 0.35
- Recommendation: Use threshold 0.35

Validation Batch:
- 90% of signs correctly detected
- Few false positives
- Missed: Small distant signs (Class 35)
```

**Action Items:**
1. ‚úÖ Model ready for testing
2. ‚ö†Ô∏è Collect more Class 22 samples
3. ‚ö†Ô∏è Improve small object detection (adjust anchor boxes)
4. ‚ö†Ô∏è Set confidence threshold to 0.35

---

## üìö Additional Resources

### View All Metrics Visually

```bash
cd YOLOv10m_training27
python3 analyze_training.py
```

### Compare Multiple Training Runs

```bash
cd /home/mnxtr/Traffic-Sign-Recognition-YOLOv10
unzip -q YOLOv10m_training22.zip
unzip -q YOLOv10m_training23.zip

# Compare confusion matrices
diff YOLOv10m_training22/confusion_matrix.png \
     YOLOv10m_training27/confusion_matrix.png
```

---

## üîç Missing Information

To get complete metrics, you would need:

1. **Model Weights (.pt file)**
   - Contains full model for inference
   - Typical location: `runs/detect/train/weights/best.pt`

2. **results.csv**
   - Numerical values for all metrics
   - Epoch-by-epoch training progress
   - Precision, Recall, mAP values

3. **Training Logs**
   - Loss curves (box, cls, dfl)
   - Learning rate schedule
   - Training/validation split details

4. **data.yaml**
   - Class names mapping
   - Dataset paths
   - Number of classes

**Where to find:** Check the Colab notebook or original training output directory.

---

**Generated:** 2025-11-18  
**Analysis Tool:** `analyze_training.py` (included in this folder)
